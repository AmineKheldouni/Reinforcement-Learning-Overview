import numpy as np
import arms
import matplotlib.pyplot as plt

# Build your own bandit problem
# First example (easy)
arm1 = arms.ArmBernoulli(0.80, random_state=np.random.randint(1, 312414))
arm2 = arms.ArmBernoulli(0.30, random_state=np.random.randint(1, 312414))
arm3 = arms.ArmBernoulli(0.10, random_state=np.random.randint(1, 312414))
MAB1 = [arm1, arm2, arm3]

# Second example
arm1 = arms.ArmBernoulli(0.48, random_state=np.random.randint(1, 312414))
arm2 = arms.ArmBernoulli(0.47, random_state=np.random.randint(1, 312414))
arm3 = arms.ArmBernoulli(0.50, random_state=np.random.randint(1, 312414))
arm4 = arms.ArmBernoulli(0.49, random_state=np.random.randint(1, 312414))
arm5 = arms.ArmBernoulli(0.49, random_state=np.random.randint(1, 312414))
MAB2 = [arm1, arm2, arm3, arm4, arm5]

# Bandit characteristics
nb_arms1 = len(MAB1)
nb_arms2 = len(MAB2)
means1 = [el.mean for el in MAB1]
means2 = [el.mean for el in MAB2]

# Display the means of your bandit (to find the best)
print('means1: {}'.format(means1))
mu_max1 = np.max(means1)
print('means2: {}'.format(means2))
mu_max2 = np.max(means2)

# Number of simulations
N = 20
# Comparison of the regret on one run of the bandit algorithm
# UCB1 algorithm implementation
def UCB1(T, MAB, rho=0.8, N=N):
    draws = np.zeros((N,T))
    rew = np.zeros((N,T))
    for i in range(N):
        Na = np.zeros((len(MAB), 1))
        Sa = np.zeros((len(MAB), 1))
        mu_hat = np.zeros((len(MAB), 1))
        for t in range(1,T+1):
            # Initialization of each arm
            if t<len(MAB):
                chosenArm = t
            else:
                values = mu_hat + rho * np.sqrt(np.log(t) / (2 * (Na+1e-10)))
                bestValue = np.max(values)
                bestArms = np.where(values == bestValue)
                chosenArm = np.random.choice(bestArms[0])
            reward = MAB[chosenArm].sample().astype(float)
            draws[i,t-1] = chosenArm
            rew[i,t-1] = reward
            Na[chosenArm] += 1
            Sa[chosenArm] += reward
            mu_hat[chosenArm] = Sa[chosenArm]/(Na[chosenArm]+1e-10)
    return (np.mean(rew,axis=0), draws)

# Thompson algorithm implementation
def TS(T, MAB, N=N):
    draws = np.zeros((N,T))
    rew = np.zeros((N,T))
    for i in range(N):
        # Success
        S = np.zeros((len(MAB), 1))
        # Failures
        F = np.zeros((len(MAB), 1))
        Na = np.zeros((len(MAB), 1))
        theta = np.zeros((len(MAB), 1))
        for t in range(T):
            for a in range(len(MAB)):
                theta[a] = np.random.beta(S[a]+1, F[a]+1)
            bestTheta = np.max(theta)
            bestArms = np.where(theta == bestTheta)
            chosenArm = np.random.choice(bestArms[0])
            Na[chosenArm] += 1
            reward = MAB[chosenArm].sample().astype(float)
            draws[i,t] = chosenArm
            rew[i,t] = reward
            if reward == 1:
                S[chosenArm] += 1
            else:
                F[chosenArm] += 1
    return (np.mean(rew,axis=0), draws)

# Kullback Leibler divergence
def kl(x,y):
    return x*np.log(x/y)+(1-x)*np.log((1-x)/(1-y))

# Lower bound: "oracle" regret curve
def oracle(t, means):
    Cp = 0
    pstar = np.max(means)
    for m in means:
        if m != pstar:
            pa = m
            Cp += (pstar-pa)/kl(pa,pstar)
    return Cp*np.log(t)

print("Complexity of MAB1: ", oracle(np.exp(1), means1))
print("Complexity of MAB2: ", oracle(np.exp(1), means2))

def naive_strategy(T, MAB, N=N):
    draws = np.zeros((N,T))
    rew = np.zeros((N,T))
    for i in range(N):
        # Number of time played
        N_a = 1e-10 * np.ones((len(MAB), 1))
        value = np.zeros((len(MAB), 1))
        for t in range(T):
            values = value/N_a
            bestValue = np.max(values)
            bestArms = np.where(values == bestValue)
            chosenArm = np.random.choice(bestArms[0])
            N_a[chosenArm] += 1
            reward = MAB[chosenArm].sample().astype(float)
            value[chosenArm] += reward
            rew[i,t] = reward
            draws[i,t] = chosenArm

    return (np.mean(rew,axis=0), draws)

# horizon
T = 6000


# Plotting cumulative regrets for comparison
rew1_ucb, draws1_ucb = UCB1(T, MAB1)
reg1_ucb = mu_max1 * np.arange(1,T+1) - np.cumsum(rew1_ucb)
rew1_ts, draws1_ts = TS(T, MAB1)
reg1_ts = mu_max1 * np.arange(1,T+1) - np.cumsum(rew1_ts)
rew1_ns, draws1_ns = naive_strategy(T, MAB1)
reg1_ns = mu_max1 * np.arange(1,T+1) - np.cumsum(rew1_ns)

rew2_ucb, draws2_ucb = UCB1(T, MAB2)
reg2_ucb = mu_max2 * np.arange(1,T+1) - np.cumsum(rew2_ucb)
rew2_ts, draws2_ts = TS(T, MAB2)
reg2_ts = mu_max2 * np.arange(1,T+1) - np.cumsum(rew2_ts)
rew2_ns, draws2_ns = naive_strategy(T, MAB2)
reg2_ns = mu_max2 * np.arange(1,T+1) - np.cumsum(rew2_ns)

f, [ax1, ax2, ax3] = plt.subplots(1, 3)
x = np.arange(1, T+1)
ax1.set_title('Regrets for first Multi-Armed Bandit example')
ax1.plot(x, reg1_ucb, label='UCB', color='blue')
ax1.plot(x, reg1_ts, label='Thompson', color='red')
ax1.plot(x, reg1_ns, label='Naive Strategy', color='gray', linestyle='dashed')
ax1.plot(x, [oracle(t, means1) for t in x], label='Oracle', color='green')
ax1.set_xlabel('Rounds', fontsize=14)
ax1.set_ylabel('Cumulative Regret', fontsize=14)
ax1.legend()

ax2.set_title('Regrets for second Multi-Armed Bandit example')
ax2.plot(x, reg2_ucb, label='UCB', color='blue')
ax2.plot(x, reg2_ts, label='Thompson', color='red')
ax2.plot(x, reg2_ns, label='Naive Strategy', color='gray', linestyle='dashed')
ax2.plot(x, [oracle(t, means2) for t in x], label='Oracle', color='green')
ax2.set_xlabel('Rounds', fontsize=14)
ax2.set_ylabel('Cumulative Regret', fontsize=14)
ax2.legend()

ax3.set_title('Regrets for second Multi-Armed Bandit example')
ax3.plot(x, reg2_ucb, label='UCB', color='blue')
ax3.plot(x, reg2_ts, label='Thompson', color='red')
ax3.set_xlabel('Rounds', fontsize=14)
ax3.set_ylabel('Cumulative Regret', fontsize=14)
ax3.legend()
plt.show()

f, [ax1, ax2] = plt.subplots(1, 2)
ax1.hist(draws1_ucb[0], label='UCB1')
ax1.set_title("Example of draws histogram for UCB1 on first MAB example")
ax2.hist(draws1_ts[0], label='Thompson sampling')
ax2.set_title("Example of draws histogram for TS on first MAB example")
plt.show()

f, [ax1, ax2] = plt.subplots(1, 2)
ax1.hist(draws2_ucb[0], label='UCB1')
ax1.set_title("Example of draws histogram for UCB1 on second MAB example")
ax2.hist(draws2_ts[0], label='Thompson sampling')
ax2.set_title("Example of draws histogram for TS on second MAB example")
plt.show()


# Plotting UCB1 with different rho values
rhoValues = [0.1, 0.2, 0.5, 0.8, 1]
f, [ax1, ax2] = plt.subplots(1, 2)
for rho in rhoValues:
    r1,d1 = UCB1(T, MAB1,rho=rho, N=20)
    r2,d2 = UCB1(T, MAB2,rho=rho, N=20)
    ax1.plot(np.arange(1, T+1), mu_max1 * np.arange(1, T + 1) - np.cumsum(r1), label='rho='+str(rho))
    ax1.set_title('Cumulative regrets of UCB1 (MAB1)')
    ax2.set_title('Cumulative regrets of UCB1 (MAB2)')
    ax2.plot(np.arange(1, T+1), mu_max1 * np.arange(1, T + 1) - np.cumsum(r2), label='rho='+str(rho))
plt.legend()
plt.show()


##################### Question 2 - Implementation #########################

# (Expected) regret curve for UCB and Thompson Sampling
arm1 = arms.ArmBernoulli(0.50, random_state=np.random.randint(1, 312414))
arm2 = arms.ArmBeta(0.3, 0.45, random_state=np.random.randint(1, 312414))
arm3 = arms.ArmExp(0.20, random_state=np.random.randint(1, 312414))
arm4 = arms.ArmExp(0.10, random_state=np.random.randint(1, 312414))
arm5 = arms.ArmBernoulli(0.1, random_state=np.random.randint(1, 312414))
arm6 = arms.ArmFinite(X = np.array([0.1, 0.3, 0.7, 0.8]), P = np.array([0.2, 0.4, 0.1, 0.3]), random_state=np.random.randint(1, 312414))

MAB = [arm1, arm2, arm3, arm4, arm5, arm6]
print("Means of diversified MAB arms (respectively)")
for a in MAB:
    print(a.mean)
# bandit : set of arms
nb_arms = len(MAB)
means = [el.mean for el in MAB]
mu_max = np.max(means)

def TSnonbinary(T, MAB, N=50):
    draws = np.zeros((N,T))
    rew = np.zeros((N,T))
    for i in range(N):
        # Success
        S = [0 for el in MAB]
        # Failures
        F = [0 for el in MAB]
        theta = [0 for el in MAB]
        for t in range(T):
            for a in range(len(MAB)):
                theta[a] = np.random.beta(S[a]+1, F[a]+1)
            bestTheta = np.max(theta)
            bestArms = np.where(theta == bestTheta)
            chosenArm = np.random.choice(bestArms[0])
            reward = MAB[chosenArm].sample().astype(float)
            rew[i,t] = reward
            draws[i,t] = chosenArm
            rt = np.random.rand() < reward
            if rt == 1:
                S[chosenArm] += 1
            else:
                F[chosenArm] += 1
    return (np.mean(rew,axis=0), draws)

T = 6000  # horizon

rew_ucb, draws_ucb = UCB1(T, MAB)
reg_ucb = mu_max * np.arange(1, T + 1) - np.cumsum(rew_ucb)
rew_ts, draws_ts = TSnonbinary(T, MAB)
reg_ts = mu_max * np.arange(1, T + 1) - np.cumsum(rew_ts)

plt.figure()
x = np.arange(1, T+1)
plt.plot(x, reg_ucb, label='UCB', color='blue')
plt.plot(x, reg_ts, label='Thompson', color='red')
plt.xlabel('Rounds', fontsize=14)
plt.ylabel('Cumulative Regret', fontsize=14)
plt.legend()
plt.show()

f, [ax1, ax2] = plt.subplots(1, 2)
ax1.hist(draws_ucb[0], label='UCB1')
ax1.set_title("Example of draws histogram for UCB1 on diversified MAB example")
ax2.hist(draws_ts[0], label='Thompson sampling')
ax2.set_title("Example of draws histogram for non-binary TS on diversified MAB example")
plt.show()
