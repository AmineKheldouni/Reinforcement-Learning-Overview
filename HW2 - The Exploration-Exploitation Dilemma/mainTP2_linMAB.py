import numpy as np
from linearmab_models import ToyLinearModel, ColdStartMovieLensModel
import matplotlib.pyplot as plt
from tqdm import tqdm

random_state = np.random.randint(0, 24532523)

### Models ###

model = ToyLinearModel(
    n_features=8,
    n_actions=20,
    random_state=random_state,
    noise=0.1)

# model = ColdStartMovieLensModel(
#     random_state=random_state,
#     noise=0.1
# )

real_theta = model.real_theta
n_a = model.n_actions
d = model.n_features

T = 6000


nb_simu = 20 # you may want to change this!

def randomPolicy(model, T, nb_simu=nb_simu):
    d = model.n_features
    n_actions = model.n_actions
    list_theta = np.zeros((nb_simu,T,d))
    R = np.zeros((nb_simu,T))
    actions = np.random.randint(0, n_actions, size=(nb_simu,T))
    lbd = 1
    for i in tqdm(range(nb_simu), desc="Simulating {}".format("random policy")):
        theta_hat = np.zeros((d,1))
        N_a = np.zeros((n_actions,1))
        A = lbd * np.eye(d)
        b = np.zeros((d,1))
        for t in range(T):
            a_t = actions[i,t]
            N_a[a_t] += 1
            r_t = model.reward(a_t).astype(float)
            phi = model.features[a_t].reshape(-1,1)
            A += phi.dot(phi.T)
            b += r_t * phi
            theta_hat = np.linalg.inv(A).dot(b)
            R[i,t] = r_t
            list_theta[i,t] = theta_hat.reshape(1,-1)
    return np.mean(R,axis=0), np.mean(list_theta,axis=0)

def epsilonGreedy(model, T, nb_simu=nb_simu, epsilon=0.1):
    d = model.n_features
    n_actions = model.n_actions
    list_theta = np.zeros((nb_simu, T,d))
    R = np.zeros((nb_simu,T))
    lbd = 1
    for i in tqdm(range(nb_simu), desc="Simulating {}".format("epsilon greedy")):
        theta_hat = np.zeros((d,1))
        N_a = np.zeros((n_actions,1))
        R_a = np.zeros((n_actions,1))
        A = lbd * np.eye(d)
        b = np.zeros((d,1))
        Q_val = np.zeros((n_actions,1))
        for t in range(T):
            if np.random.rand() < epsilon:
                a_t = np.random.randint(0,n_actions)
            else:
                bestValue = np.max(Q_val)
                bestActions = np.where(Q_val == bestValue)
                a_t = np.random.choice(bestActions[0])
            N_a[a_t] += 1
            r_t = model.reward(a_t).astype(float)
            R_a[a_t] += r_t
            Q_val[a_t] = R_a[a_t]/N_a[a_t]
            phi = model.features[a_t].reshape(-1,1)
            A += phi.dot(phi.T)
            b += r_t * phi
            theta_hat = np.linalg.inv(A).dot(b)
            R[i,t] = r_t
            list_theta[i,t] = theta_hat.reshape(1,-1)

    return np.mean(R, axis=0), np.mean(list_theta, axis=0)

def linearUCB(model, T, nb_simu=nb_simu, alpha= 1 + np.sqrt(np.log(2/0.01)/2)):
    d = model.n_features
    R = np.zeros((nb_simu,T))
    d = model.n_features
    list_theta = np.zeros((nb_simu,T,d))
    lbd = 1
    n_actions = model.n_actions
    for i in tqdm(range(nb_simu), desc="Simulating {}".format("LinUCB")):
        theta_hat = np.zeros((d,1))
        N_a = np.zeros((n_actions,1))
        r_hat = np.zeros((n_actions, 1))
        R_a = np.zeros((n_actions,1))
        A = lbd * np.eye(d)
        b = np.zeros((d,1))
        Q_val = np.zeros((n_actions,1))
        beta = np.zeros((n_actions,1))
        for t in range(T):
            for a in range(n_actions):
                beta[a] = alpha * np.sqrt(model.features[a].reshape(1,-1).dot(np.linalg.inv(A)).dot(model.features[a].reshape(-1,1)))
                r_hat[a] = model.features[a].dot(theta_hat) + beta[a]
            bestReward = np.max(r_hat)
            bestActions = np.where(r_hat == bestReward)
            a_t = np.random.choice(bestActions[0])
            N_a[a_t] += 1
            r_t = model.reward(a_t).astype(float)
            phi = model.features[a_t].reshape(-1,1)
            A += phi.dot(phi.T)
            b += r_t * phi
            theta_hat = np.linalg.inv(A).dot(b)
            R[i,t] = r_t
            list_theta[i,t] = theta_hat.reshape(1,-1)

    return np.mean(R, axis=0), np.mean(list_theta, axis=0)

Rr, Tr = randomPolicy(model, T, nb_simu)
Reps, Teps = epsilonGreedy(model, T, nb_simu)
Rucb, Tucb = linearUCB(model, T, nb_simu)
mean_norms_ucb =  np.linalg.norm(Tucb - model.real_theta.reshape(1,-1), 2, axis=1)
mean_regret_ucb = model.best_arm_reward() - Rucb
mean_norms_random =  np.linalg.norm(Tr - model.real_theta.reshape(1,-1), 2, axis=1)
mean_regret_random = model.best_arm_reward() - Rr
mean_norms_eps =  np.linalg.norm(Teps - model.real_theta.reshape(1,-1), 2, axis=1)
mean_regret_eps = model.best_arm_reward() - Reps

plt.figure(1)
plt.subplot(121)
plt.plot(mean_norms_ucb, label="linUCB")
plt.plot(mean_norms_eps, label="Epsilon Greedy")
plt.plot(mean_norms_random, label="Random Strategy")
plt.ylabel('d(theta, theta_hat)', fontsize=14)
plt.xlabel('Rounds', fontsize=14)
plt.legend()

plt.subplot(122)
plt.plot(mean_regret_ucb.cumsum(), label="linUCB")
plt.plot(mean_regret_eps.cumsum(), label="Epsilon Greedy")
plt.plot(mean_regret_random.cumsum(), label="Random Strategy")
plt.ylabel('Cumulative Regret', fontsize=14)
plt.xlabel('Rounds', fontsize=14)
plt.legend()
plt.show()

epsilonValues = [0.002, 0.1, 0.5, 0.8]
alphaValues = [0, 0.5, 2.63, 7]

plt.figure(1)
plt.subplot(121)
for eps in epsilonValues:
    Reps, Teps = epsilonGreedy(model, T, nb_simu, epsilon=eps)
    mean_regret_eps = model.best_arm_reward() - Reps
    plt.plot(mean_regret_eps.cumsum(), label="eps = " + str(eps))

plt.legend()

plt.subplot(122)
for alpha in alphaValues:
    Rucb, Tucb = linearUCB(model, T, nb_simu, alpha=alpha)
    mean_regret_ucb = model.best_arm_reward() - Rucb
    plt.plot(mean_regret_ucb.cumsum(), label="alpha = " + str(alpha))

plt.legend()
plt.show()
